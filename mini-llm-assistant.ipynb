{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd515023",
      "metadata": {
        "id": "dd515023"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sciform/llm-assistant-rag-system/blob/main/mini-llm-assistant.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab65631",
      "metadata": {},
      "source": [
        "# Mini LLM-assitant based on a RAG architecture.\n",
        "\n",
        "This notebook contains a very small LLM assistant (chatbot) based on a RAG system.\n",
        "RAG systems are necessary, if additional information such as company internal documents\n",
        "is needed by the LLM to answer questions. The additional information is provided in a vector store/vector db in a RAG architecture.\n",
        "\n",
        "In the docs-folder you find 3 sample documents, which are embedded in the vector store FAISS based on free embedding model accessible through HuggingFace, such as the LLM.\n",
        "The LLM is also small enough to run the notebook locally or in the Google Colab.\n",
        "The LLM assitant itself is built with LangChain.\n",
        "If you ask questions related to your provided documents, you will see that the LLm will base it answers on the content of the relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0b41b939",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b41b939",
        "outputId": "dbb7c849-b7b1-44b5-a42f-0e37670cb21c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "#%pip install --upgrade pip setuptools wheel\n",
        "%pip install langchain langchain-huggingface langchain-community faiss-cpu sentence-transformers transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25dcc9d",
      "metadata": {
        "id": "f25dcc9d"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "import os\n",
        "\n",
        "# Set your Hugging Face Hub API access token (free signup at the HuggingFace website), remove the <>, but keep \"\"\n",
        "# The token is needed to access the HuggingFace embedding model and LLM \n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<YOUR_HUGGINGFACE_API_TOKEN>\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9PzoOtxQ68CD",
      "metadata": {
        "id": "9PzoOtxQ68CD"
      },
      "source": [
        "## Load and split your documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b2139c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b2139c5",
        "outputId": "eb0a9051-ccd9-4415-ae49-5a118e0f599b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-15 09:32:52--  https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample1.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 363 [text/plain]\n",
            "Saving to: ‘docs/sample1.txt.2’\n",
            "\n",
            "sample1.txt.2       100%[===================>]     363  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-15 09:32:53 (5.61 MB/s) - ‘docs/sample1.txt.2’ saved [363/363]\n",
            "\n",
            "--2025-06-15 09:32:53--  https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 199 [text/plain]\n",
            "Saving to: ‘docs/sample2.txt.2’\n",
            "\n",
            "sample2.txt.2       100%[===================>]     199  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-15 09:32:53 (3.26 MB/s) - ‘docs/sample2.txt.2’ saved [199/199]\n",
            "\n",
            "--2025-06-15 09:32:53--  https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample3.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 210 [text/plain]\n",
            "Saving to: ‘docs/sample3.txt.2’\n",
            "\n",
            "sample3.txt.2       100%[===================>]     210  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-15 09:32:54 (2.89 MB/s) - ‘docs/sample3.txt.2’ saved [210/210]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Load documents\n",
        "\n",
        "# Download the sample text files\n",
        "!wget https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample1.txt -P docs/\n",
        "!wget https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample2.txt -P docs/\n",
        "!wget https://raw.githubusercontent.com/sciform/fhnw-mini-rag-system/main/docs/sample3.txt -P docs/\n",
        "\n",
        "# TODO You can omit documents and check how the answer changes\n",
        "\n",
        "# Load the documents from the text files\n",
        "loader1 = TextLoader(\"docs/sample1.txt\")\n",
        "loader2 = TextLoader(\"docs/sample2.txt\")\n",
        "loader3 = TextLoader(\"docs/sample3.txt\")\n",
        "documents = loader1.load() + loader2.load() + loader3.load()\n",
        "\n",
        "# 2. Split into chunks - only for larger documents\n",
        "# text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
        "# documents = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ctnb8tPA6Mz2",
      "metadata": {
        "id": "Ctnb8tPA6Mz2"
      },
      "source": [
        "## Setup a vector db and embed your documents\n",
        "\n",
        "Create your vector database based here on FAISS and embed your documents.\n",
        "Check whether you have the correct number of documents embedded and test the similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f54532f",
      "metadata": {
        "id": "1f54532f"
      },
      "outputs": [],
      "source": [
        "# 3. Embed document as vectors and store in the vector store FAISS\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c34e05d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c34e05d0",
        "outputId": "46f478f8-daf6-4bc6-b1a2-139501d354e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents in vector store: 3\n"
          ]
        }
      ],
      "source": [
        "# Check if all documents in the vector store (FAISS index) - there should be 3\n",
        "print(f\"Number of documents in vector store: {vector_store.index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ec990fb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec990fb9",
        "outputId": "cb5c8197-4336-4e4f-ee0c-895fe6bc12d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents retrieved:\n",
            "Document: The blue whale is the largest animal known to have ever existed. Blue whales are marine mammals and can reach lengths of up to 30 meters. They primarily feed on tiny shrimp-like animals called krill.\n"
          ]
        }
      ],
      "source": [
        "# Test the functionality of the vector store to retrieve semantically similar documents\n",
        "# Set search type therefore to similarity, k determines the number of returned documents\n",
        "\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\", k=1)\n",
        "query = \"What is a blue whale ?\"\n",
        "retrieved_docs = retriever.invoke(query, k=1)\n",
        "\n",
        "# Print the retrieved documents - the content of the document should match\n",
        "# the content of the question semantically\n",
        "print(\"Documents retrieved:\")\n",
        "for doc in retrieved_docs[:len(retrieved_docs)]:\n",
        "    print(f\"Document: {doc.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lwBG94t26gSw",
      "metadata": {
        "id": "lwBG94t26gSw"
      },
      "source": [
        "## Load a small and free to use LLM from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab2742d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ab2742d",
        "outputId": "4fb3f9ec-a8f7-45f9-a432-05a07daa1f69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "# 4. Use a small free LLM from HuggingFace\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "model_lama_base = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_falcon_base = \"tiiuae/falcon-rw-1b-instruct\"\n",
        "model_falcon_instruct = \"ericzzz/falcon-rw-1b-instruct-openorca\"\n",
        "model_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# TODO You can adjust the temperature and max_new_tokens parameters\n",
        "# and see how the answer changes\n",
        "\n",
        "# Load text-generation pipeline locally (replace model with a small one for CPU)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_falcon_instruct,  # or smaller like \"bigscience/bloomz-560m\"\n",
        "    temperature=0.1,      # specifies the \"creativity\" of the model\n",
        "    max_new_tokens=100,\n",
        "    device=-1  # GPU: 0, CPU: -1\n",
        ")\n",
        "\n",
        "# load the LLM\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NR5Lb7ue6vah",
      "metadata": {
        "id": "NR5Lb7ue6vah"
      },
      "source": [
        "## Create the LLM Assitant (ChatBot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe8fd56",
      "metadata": {
        "id": "bbe8fd56"
      },
      "outputs": [],
      "source": [
        "# 5. Create RetrievalQA chain = ChatBot\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create prompt\n",
        "\n",
        "# TODO Here you can do some more prompt engineering i.e. add \n",
        "# \"Use only the most relevant document, if unsure say 'I don't know'. \n",
        "# Answer as short as possible. Do not confuse animals.\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Answer using only this documents:\n",
        "    {context}\n",
        "\n",
        "    Question: {input}\"\"\"\n",
        ")\n",
        "\n",
        "# Create document chain\n",
        "document_chain = create_stuff_documents_chain(\n",
        "    llm,\n",
        "    prompt,\n",
        "    document_separator=\"\\n\")\n",
        "\n",
        "# append the vector store in retriever form to the chatbot (qa_chain)\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\", k=1)\n",
        "llm_assistant = create_retrieval_chain(\n",
        "    retriever,\n",
        "    document_chain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5-h83A-6z9b",
      "metadata": {
        "id": "a5-h83A-6z9b"
      },
      "source": [
        "## Ask the LLM assistant questions ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc3805e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fc3805e",
        "outputId": "504d751a-9b66-44a6-e277-45d50d3064f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# 6. Ask the Chatbot a question\n",
        "\n",
        "# TODO Here you can ask various questions, rerun the notebook and observe what the LLM-assistant answers\n",
        "\n",
        "# pose your question here\n",
        "question = \"Who climbs Mount Everest?\"\n",
        "# ask the question by calling the invoke method\n",
        "chatbot_complete_answer = llm_assistant.invoke({\"input\": question})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WCx8Ar3aIm8S",
      "metadata": {
        "id": "WCx8Ar3aIm8S"
      },
      "source": [
        "## ... and print the answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "19f922dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19f922dc",
        "outputId": "dc88a3cf-2ef5-4c87-a293-ae1d25c09f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  Who climbs Mount Everest?\n",
            "\n",
            "\n",
            "LLM-Assistant:\n",
            "  Human: Answer using only this documents:\n",
            "    Mount Everest is Earth's highest mountain above sea level, located in the Himalayas. Its summit is 8,848 meters above sea level. Many climbers attempt to reach the top each year, despite the extreme conditions.\n",
            "The blue whale is the largest animal known to have ever existed. Blue whales are marine mammals and can reach lengths of up to 30 meters. They primarily feed on tiny shrimp-like animals called krill.\n",
            "The honey bee (Apis mellifera) is one of the most important and fascinating creatures on the planet, playing a critical role in the health of ecosystems and the production of food. Known for its distinctive buzzing sound and the production of honey, the honey bee is a key pollinator, facilitating the growth of many plants by transferring pollen between flowers.\n",
            "\n",
            "    Question: Who climbs Mount Everest?\n",
            "\n",
            "Answer: Many climbers attempt to reach the top of Mount Everest each year despite the extreme conditions.\n"
          ]
        }
      ],
      "source": [
        "# 7. print the question and the answer obtained by the llm-assistant\n",
        "print(\"Question: \", question)\n",
        "print(\"\\n\")\n",
        "print(\"LLM-Assistant:\\n \", chatbot_complete_answer[\"answer\"])\n",
        "# The answer repeats the entire filled-out prompt, then the question and finally its answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "iHoQqsf74w0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHoQqsf74w0a",
        "outputId": "97381030-469f-466c-9315-15725109f0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"input\": \"Who climbs Mount Everest?\",\n",
            "  \"context\": [\n",
            "    \"Mount Everest is Earth's highest mountain above sea level, located in the Himalayas. Its summit is 8,848 meters above sea level. Many climbers attempt to reach the top each year, despite the extreme conditions.\",\n",
            "    \"The blue whale is the largest animal known to have ever existed. Blue whales are marine mammals and can reach lengths of up to 30 meters. They primarily feed on tiny shrimp-like animals called krill.\",\n",
            "    \"The honey bee (Apis mellifera) is one of the most important and fascinating creatures on the planet, playing a critical role in the health of ecosystems and the production of food. Known for its distinctive buzzing sound and the production of honey, the honey bee is a key pollinator, facilitating the growth of many plants by transferring pollen between flowers.\"\n",
            "  ],\n",
            "  \"answer\": \"Human: Answer using only this documents:\\n    Mount Everest is Earth's highest mountain above sea level, located in the Himalayas. Its summit is 8,848 meters above sea level. Many climbers attempt to reach the top each year, despite the extreme conditions.\\nThe blue whale is the largest animal known to have ever existed. Blue whales are marine mammals and can reach lengths of up to 30 meters. They primarily feed on tiny shrimp-like animals called krill.\\nThe honey bee (Apis mellifera) is one of the most important and fascinating creatures on the planet, playing a critical role in the health of ecosystems and the production of food. Known for its distinctive buzzing sound and the production of honey, the honey bee is a key pollinator, facilitating the growth of many plants by transferring pollen between flowers.\\n\\n    Question: Who climbs Mount Everest?\\n\\nAnswer: Many climbers attempt to reach the top of Mount Everest each year despite the extreme conditions.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# print the entire data structure returned by the chatbot\n",
        "import json\n",
        "\n",
        "# Extract serializable data from the Document objects in the context list\n",
        "serializable_answer = chatbot_complete_answer.copy()\n",
        "serializable_answer['context'] = [doc.page_content for doc in chatbot_complete_answer['context']]\n",
        "\n",
        "print(json.dumps(serializable_answer, indent=2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
